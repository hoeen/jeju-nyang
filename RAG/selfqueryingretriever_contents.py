from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableMap

from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import OpenAIEmbeddings

import os

# TODO : ìƒëŒ€ê²½ë¡œë¡œ ë°”ê¾¸ê¸° 


os.environ["OPENAI_API_KEY"] = api_key

VECTORSTORE_DIR = "../vectorstores/visitjeju_db_place_food_shopping_stay"
# Initialize LLM and vectorstores
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

# Define a function to format retrieved documents
def format_docs(docs: iter):
    print("\n\n".join([doc.page_content for doc in docs]))
    # print(docs)
    return "\n\n".join([doc.page_content for doc in docs])

# Define the structured query model
class StructuredQueryPlaces(BaseModel):
    """ì§ˆë¬¸ì—ì„œ ì¥ì†Œ ì •ë³´ë¥¼ ë°˜í™˜"""
    place: str = Field(..., description="ì¥ì†Œ, ì˜ˆ) ì„±ì‚°ì¼ì¶œë´‰, ìš°ë„")

class SelfQueryingRetrieverContents:
    def __init__(self, llm, vectorstore_dir = None):
       
        # define llm 
        self.llm = llm

        # import vectorstores
        self.context_vectorstore = Chroma(persist_directory=vectorstore_dir, collection_name="visitjeju_contexts", embedding_function=OpenAIEmbeddings())
        self.title_vectorstore = Chroma(persist_directory=vectorstore_dir, collection_name="visitjeju_titles", embedding_function=OpenAIEmbeddings())
        
        # Define the system prompt for extracting place information
        system = """
            ë‹¹ì‹ ì€ ì œì£¼ë„ì˜ ì—¬í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤.
            ì§ˆë¬¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ ì£¼ì–´ì§€ëŠ” ì¿¼ë¦¬ë¡œ ë°˜í™˜í•˜ì‹­ì‹œì˜¤. 
            """
        self.prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{question}")])

        # Define the template for the RAG prompt
        template = """
            ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì¶”ì²œ ê²°ê³¼ë¥¼ ì„¤ëª…í•´ì£¼ëŠ” ì¹œì ˆí•˜ê³  ê·€ì—¬ìš´ ì¶”ì²œëƒ¥ğŸ˜½ì…ë‹ˆë‹¤. 
            ## ì§€ì‹œì‚¬í•­ :
            - ë§ ëì— ê³ ì–‘ì´ì²˜ëŸ¼ 'ëƒ¥' ì„ ë„£ê±°ë‚˜ ê³ ì–‘ì´ ì´ëª¨í‹°ì½˜ì„ í™œìš©í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ë©´ 'í–ˆë‹¤ëƒ¥ğŸ±âœ¨'
            - ë‹¹ì‹ ì€ ì§ˆë¬¸-ì‘ë‹µ ì‘ì—…ì„ ìœ„í•œ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            - ë‹¤ìŒì˜ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì‹­ì‹œì˜¤.
            - ì»¨í…ìŠ¤íŠ¸ ë‚´ì˜ ì¶”ì²œëœ ë…ë¦½ëœ ì¥ì†Œ ì´ë¦„ê³¼ ì„¤ëª…ì„ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.
            - ì‘ë‹µì— ì–´ìš¸ë¦¬ëŠ” ì´ëª¨í‹°ì½˜ì„ 2ê°œ ì´ë‚´ë¡œ ë„£ì–´ë„ ë©ë‹ˆë‹¤.
            - ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ì— ë‚´ìš©ì´ ì—†ì„ ê²½ìš°, ì˜ ëª¨ë¥´ë‹ˆ ì§ˆë¬¸ì„ ë°”ê¿”ì„œ í•´ ë‹¬ë¼ê³  ìš”ì²­í•˜ì„¸ìš”.

            ì§ˆë¬¸: {question}
            ì»¨í…ìŠ¤íŠ¸: {context}
            ë‹µë³€: 
        """
        self.ragprompt = ChatPromptTemplate.from_template(template)
        self.structured_llm = llm.with_structured_output(StructuredQueryPlaces)

        # Combine the query analyzer and retriever into one chain
        self.query_analyzer = {"question": RunnablePassthrough()} | self.prompt | self.structured_llm 


    # ì œëª© ì¶”ì¶œ ë° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ í†µí•©í•œ RAG ì²´ì¸ ì •ì˜
    def create_rag_chain(self, filter_context: str):
        retrieved_context = self.context_vectorstore.as_retriever(
            search_type="similarity_score_threshold", 
            search_kwargs={"score_threshold": 0.8}
        ).invoke(filter_context)
        print('retrieved_context',retrieved_context)
        if not retrieved_context: 
            return None
        # print('context.metadata[title], context.page_content')
        # breakpoint()
        retrieved_context_chosen = [(context.metadata['title'], context.page_content) for context in retrieved_context]  # ê°€ì¥ ê°€ê¹Œìš´ í•˜ë‚˜ë§Œ ì„ íƒ
        
        # breakpoint()
        print('retrieved_context_chosen:', retrieved_context_chosen)
        # context_retriever = self.context_vectorstore.as_retriever(search_kwargs={"filter": {"title": retrieved_title}})
        print("\n".join([f"ì¥ì†Œëª…: {title}, ì„¤ëª…: {context}" for (title, context) in retrieved_context_chosen]))
        rag_chain = (
            RunnableMap(
                {
                    "question": RunnablePassthrough(),
                    "context": lambda x: "\n".join([f"ì¥ì†Œëª…: {title}, ì„¤ëª…: {context}" for (title, context) in retrieved_context_chosen])
                }
            )
            | self.ragprompt
            | self.llm
            | StrOutputParser()
        )
        return rag_chain


if __name__ == "__main__":
    
    retriever = SelfQueryingRetrieverContents(llm, VECTORSTORE_DIR)
    query = input('Question: ')
    while query != 'quit':
        place_info = retriever.query_analyzer.invoke({"question": query})
        # breakpoint()
        rag_chain = retriever.create_rag_chain(place_info.place) 
        if rag_chain is not None:
            result = rag_chain.invoke(query)
            print(result)  # retriever ê²°ê³¼ context í™•ì¸ 
            query = input('Question: ')
        else:
            query = input('ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nQuestion: ')
