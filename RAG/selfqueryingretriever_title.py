from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableMap

from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import OpenAIEmbeddings

import os

# TODO : ?ï¿½ï¿½???ê²½ë¡œï¿½? ë°”ê¾¸ï¿½? 


VECTORSTORE_DIR = "../vectorstores/visitjeju_db_place_food_shopping_stay"
# Initialize LLM and vectorstores
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

# Define a function to format retrieved documents
def format_docs(docs: iter):
    print("\n\n".join([doc.page_content for doc in docs]))
    # print(docs)
    return "\n\n".join([doc.page_content for doc in docs])

# Define the structured query model
class StructuredQueryPlaces(BaseModel):
    """ì§ˆë¬¸?ï¿½ï¿½?ï¿½ï¿½ ?ï¿½ï¿½?ï¿½ï¿½ ?ï¿½ï¿½ë³´ï¿½?? ë°˜í™˜"""
    place: str = Field(..., description="ì¥ì†Œ, ì˜ˆ) ì„±ì‚°ì¼ì¶œë´‰, ìš°ë„")

class SelfQueryingRetrieverTitle:
    def __init__(self, llm, vectorstore_dir = None):
       
        # define llm 
        self.llm = llm

        # import vectorstores
        self.context_vectorstore = Chroma(persist_directory=vectorstore_dir, collection_name="visitjeju_contexts", embedding_function=OpenAIEmbeddings())
        self.title_vectorstore = Chroma(persist_directory=vectorstore_dir, collection_name="visitjeju_titles", embedding_function=OpenAIEmbeddings())
        
        # Define the system prompt for extracting place information
        system = """
            ë‹¹ì‹ ì€ ì œì£¼ë„ì˜ ì—¬í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤.
            ì§ˆë¬¸ì—ì„œ ì¥ì†Œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ ì£¼ì–´ì§€ëŠ” ì¿¼ë¦¬ë¡œ ë°˜í™˜í•˜ì‹­ì‹œì˜¤. 
            ì¥ì†Œ ì •ë³´ ì´ì™¸ì— ë‹¤ë¥¸ ì •ë³´ëŠ” ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”."""
        self.prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{question}")])

        # Define the template for the RAG prompt
        template = """
            ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì¶”ì²œ ê²°ê³¼ë¥¼ ì„¤ëª…í•´ì£¼ëŠ” ì¹œì ˆí•˜ê³  ê·€ì—¬ìš´ ì¶”ì²œëƒ¥ğŸ˜½ì…ë‹ˆë‹¤. 

            ## ì§€ì‹œì‚¬í•­ :
            - ë§ ëì— ê³ ì–‘ì´ì²˜ëŸ¼ 'ëƒ¥' ì„ ë„£ê±°ë‚˜ ê³ ì–‘ì´ ì´ëª¨í‹°ì½˜ì„ í™œìš©í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ë©´ 'í–ˆë‹¤ëƒ¥ğŸ±âœ¨'
            - ë‹¤ìŒì˜ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ì„¸ ì¤„ ì´ë‚´ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤.
            - ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ì— ë‚´ìš©ì´ ì—†ì„ ê²½ìš°, ì˜ ëª¨ë¥´ë‹ˆ ì§ˆë¬¸ì„ ë°”ê¿”ì„œ í•´ ë‹¬ë¼ê³  ìš”ì²­í•˜ì„¸ìš”.
            - ì¹œê·¼í•œ ë§íˆ¬ë¡œ ë‹µí•˜ê³ , ì¤‘ê°„ì¤‘ê°„ ì–´ìš¸ë¦¬ëŠ” ì´ëª¨í‹°ì½˜ì„ í•œê°œì”© í•¨ê»˜ ì¨ ì£¼ì„¸ìš”.

            ì§ˆë¬¸: {question}
            ì»¨í…ìŠ¤íŠ¸: {context}
            ë‹µë³€:
        """
        self.ragprompt = ChatPromptTemplate.from_template(template)
        self.structured_llm = llm.with_structured_output(StructuredQueryPlaces)

        # Combine the query analyzer and retriever into one chain
        self.query_analyzer = {"question": RunnablePassthrough()} | self.prompt | self.structured_llm 


    # ì œëª© ì¶”ì¶œ ë° ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ í†µí•©í•œ RAG ì²´ì¸ ì •ì˜
    def create_rag_chain(self, filter_title: str):
        retrieved_title = self.title_vectorstore.as_retriever(
            search_type="similarity_score_threshold", 
            search_kwargs={"score_threshold": 0.7}
        ).invoke(filter_title)
        print('retrieved_title:', retrieved_title)
        if not retrieved_title: 
            return None

        retrieved_title = retrieved_title[0].page_content  # ê°€ì¥ ê°€ê¹Œìš´ í•˜ë‚˜ë§Œ ì„ íƒ
        # breakpoint()
        context_retriever = self.context_vectorstore.as_retriever(search_kwargs={"filter": {"title": retrieved_title}},k=6)
        rag_chain = (
            RunnableMap(
                {
                    "question": RunnablePassthrough(),
                    "context": context_retriever | format_docs,
                }
            )
            | self.ragprompt
            | self.llm
            | StrOutputParser()
        )
        return rag_chain


if __name__ == "__main__":
    
    retriever = SelfQueryingRetrieverTitle(llm, VECTORSTORE_DIR)
    query = input('Question: ')
    while query != 'quit':
        place_info = retriever.query_analyzer.invoke({"question": query})
        # breakpoint()
        rag_chain = retriever.create_rag_chain(place_info.place) 
        if rag_chain is not None:
            result = rag_chain.invoke(query)
            print(result)  # retriever ê²°ê³¼ context ?ï¿½ï¿½?ï¿½ï¿½ 
            query = input('Question: ')
        else:
            query = input('ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nQuestion: ')
